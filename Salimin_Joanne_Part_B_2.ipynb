{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NFVxWZGJxprU"
   },
   "source": [
    "# Question B2 (10 marks)\n",
    "In Question B1, we used the Category Embedding model. This creates a feedforward neural network in which the categorical features get learnable embeddings. In this question, we will make use of a library called Pytorch-WideDeep. This library makes it easy to work with multimodal deep-learning problems combining images, text, and tables. We will just be utilizing the deeptabular component of this library through the TabMlp network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "EycCozG06Duu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch-widedeep\n",
      "  Using cached pytorch_widedeep-1.6.3-py3-none-any.whl (21.9 MB)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in c:\\users\\joann\\coding projects\\ipynb dump\\ipynb-dump\\lib\\site-packages (from pytorch-widedeep) (1.5.2)\n",
      "Requirement already satisfied: wrapt in c:\\users\\joann\\coding projects\\ipynb dump\\ipynb-dump\\lib\\site-packages (from pytorch-widedeep) (1.16.0)\n",
      "Requirement already satisfied: torchvision>=0.15.0 in c:\\users\\joann\\coding projects\\ipynb dump\\ipynb-dump\\lib\\site-packages (from pytorch-widedeep) (0.19.1)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\users\\joann\\coding projects\\ipynb dump\\ipynb-dump\\lib\\site-packages (from pytorch-widedeep) (2.4.1)\n",
      "Requirement already satisfied: transformers in c:\\users\\joann\\coding projects\\ipynb dump\\ipynb-dump\\lib\\site-packages (from pytorch-widedeep) (4.45.1)\n",
      "Requirement already satisfied: scipy<=1.12.0,>=1.7.3 in c:\\users\\joann\\coding projects\\ipynb dump\\ipynb-dump\\lib\\site-packages (from pytorch-widedeep) (1.12.0)\n",
      "Collecting fastparquet>=0.8.1\n",
      "  Using cached fastparquet-2024.5.0-cp39-cp39-win_amd64.whl (672 kB)\n",
      "Collecting sentencepiece\n",
      "  Using cached sentencepiece-0.2.0-cp39-cp39-win_amd64.whl (991 kB)\n",
      "Collecting sentence-transformers\n",
      "  Using cached sentence_transformers-3.1.1-py3-none-any.whl (245 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\joann\\coding projects\\ipynb dump\\ipynb-dump\\lib\\site-packages (from pytorch-widedeep) (4.66.5)\n",
      "Requirement already satisfied: imutils in c:\\users\\joann\\coding projects\\ipynb dump\\ipynb-dump\\lib\\site-packages (from pytorch-widedeep) (0.5.4)\n",
      "Collecting opencv-contrib-python\n",
      "  Using cached opencv_contrib_python-4.10.0.84-cp37-abi3-win_amd64.whl (45.5 MB)\n",
      "Collecting pyarrow\n",
      "  Using cached pyarrow-17.0.0-cp39-cp39-win_amd64.whl (25.1 MB)\n",
      "Requirement already satisfied: torchmetrics in c:\\users\\joann\\coding projects\\ipynb dump\\ipynb-dump\\lib\\site-packages (from pytorch-widedeep) (1.2.1)\n",
      "Requirement already satisfied: pandas>=1.3.5 in c:\\users\\joann\\coding projects\\ipynb dump\\ipynb-dump\\lib\\site-packages (from pytorch-widedeep) (2.2.3)\n",
      "Collecting spacy\n",
      "  Using cached spacy-3.7.6-cp39-cp39-win_amd64.whl (12.2 MB)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.21.6 in c:\\users\\joann\\coding projects\\ipynb dump\\ipynb-dump\\lib\\site-packages (from pytorch-widedeep) (1.26.4)\n",
      "Requirement already satisfied: einops in c:\\users\\joann\\coding projects\\ipynb dump\\ipynb-dump\\lib\\site-packages (from pytorch-widedeep) (0.7.0)\n",
      "Collecting gensim\n",
      "  Using cached gensim-4.3.3-cp39-cp39-win_amd64.whl (24.0 MB)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\joann\\coding projects\\ipynb dump\\ipynb-dump\\lib\\site-packages (from scikit-learn>=1.0.2->pytorch-widedeep) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\joann\\coding projects\\ipynb dump\\ipynb-dump\\lib\\site-packages (from scikit-learn>=1.0.2->pytorch-widedeep) (3.5.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\joann\\coding projects\\ipynb dump\\ipynb-dump\\lib\\site-packages (from torchvision>=0.15.0->pytorch-widedeep) (10.4.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\joann\\coding projects\\ipynb dump\\ipynb-dump\\lib\\site-packages (from torch>=2.0.0->pytorch-widedeep) (4.12.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\joann\\coding projects\\ipynb dump\\ipynb-dump\\lib\\site-packages (from torch>=2.0.0->pytorch-widedeep) (3.1.4)\n",
      "Requirement already satisfied: sympy in c:\\users\\joann\\coding projects\\ipynb dump\\ipynb-dump\\lib\\site-packages (from torch>=2.0.0->pytorch-widedeep) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\joann\\coding projects\\ipynb dump\\ipynb-dump\\lib\\site-packages (from torch>=2.0.0->pytorch-widedeep) (3.2.1)\n",
      "Requirement already satisfied: fsspec in c:\\users\\joann\\coding projects\\ipynb dump\\ipynb-dump\\lib\\site-packages (from torch>=2.0.0->pytorch-widedeep) (2024.9.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\joann\\coding projects\\ipynb dump\\ipynb-dump\\lib\\site-packages (from torch>=2.0.0->pytorch-widedeep) (3.16.1)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\joann\\coding projects\\ipynb dump\\ipynb-dump\\lib\\site-packages (from transformers->pytorch-widedeep) (0.20.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\joann\\coding projects\\ipynb dump\\ipynb-dump\\lib\\site-packages (from transformers->pytorch-widedeep) (24.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\joann\\coding projects\\ipynb dump\\ipynb-dump\\lib\\site-packages (from transformers->pytorch-widedeep) (0.4.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\joann\\coding projects\\ipynb dump\\ipynb-dump\\lib\\site-packages (from transformers->pytorch-widedeep) (0.25.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\joann\\coding projects\\ipynb dump\\ipynb-dump\\lib\\site-packages (from transformers->pytorch-widedeep) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\joann\\coding projects\\ipynb dump\\ipynb-dump\\lib\\site-packages (from transformers->pytorch-widedeep) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\joann\\coding projects\\ipynb dump\\ipynb-dump\\lib\\site-packages (from transformers->pytorch-widedeep) (2.32.3)\n",
      "Collecting cramjam>=2.3\n",
      "  Using cached cramjam-2.8.4-cp39-none-win_amd64.whl (2.1 MB)\n",
      "Requirement already satisfied: colorama; platform_system == \"Windows\" in c:\\users\\joann\\coding projects\\ipynb dump\\ipynb-dump\\lib\\site-packages (from tqdm->pytorch-widedeep) (0.4.6)\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in c:\\users\\joann\\coding projects\\ipynb dump\\ipynb-dump\\lib\\site-packages (from torchmetrics->pytorch-widedeep) (0.11.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\joann\\coding projects\\ipynb dump\\ipynb-dump\\lib\\site-packages (from pandas>=1.3.5->pytorch-widedeep) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\joann\\coding projects\\ipynb dump\\ipynb-dump\\lib\\site-packages (from pandas>=1.3.5->pytorch-widedeep) (2024.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\joann\\coding projects\\ipynb dump\\ipynb-dump\\lib\\site-packages (from pandas>=1.3.5->pytorch-widedeep) (2024.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\joann\\coding projects\\ipynb dump\\ipynb-dump\\lib\\site-packages (from spacy->pytorch-widedeep) (49.2.1)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11\n",
      "  Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4\n",
      "  Using cached pydantic-2.9.2-py3-none-any.whl (434 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Using cached cymem-2.0.8-cp39-cp39-win_amd64.whl (39 kB)\n",
      "Collecting typer<1.0.0,>=0.3.0\n",
      "  Using cached typer-0.12.5-py3-none-any.whl (47 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Using cached catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Using cached langcodes-3.4.1-py3-none-any.whl (182 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Using cached srsly-2.4.8-cp39-cp39-win_amd64.whl (483 kB)\n",
      "Collecting thinc<8.3.0,>=8.2.2\n",
      "  Using cached thinc-8.2.5-cp39-cp39-win_amd64.whl (1.5 MB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Using cached spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Using cached murmurhash-1.0.10-cp39-cp39-win_amd64.whl (25 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0\n",
      "  Using cached weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1\n",
      "  Using cached wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Using cached preshed-3.0.9-cp39-cp39-win_amd64.whl (122 kB)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Using cached smart_open-7.0.4-py3-none-any.whl (61 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\joann\\coding projects\\ipynb dump\\ipynb-dump\\lib\\site-packages (from jinja2->torch>=2.0.0->pytorch-widedeep) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\joann\\coding projects\\ipynb dump\\ipynb-dump\\lib\\site-packages (from sympy->torch>=2.0.0->pytorch-widedeep) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\joann\\coding projects\\ipynb dump\\ipynb-dump\\lib\\site-packages (from requests->transformers->pytorch-widedeep) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\joann\\coding projects\\ipynb dump\\ipynb-dump\\lib\\site-packages (from requests->transformers->pytorch-widedeep) (2.2.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\joann\\coding projects\\ipynb dump\\ipynb-dump\\lib\\site-packages (from requests->transformers->pytorch-widedeep) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\joann\\coding projects\\ipynb dump\\ipynb-dump\\lib\\site-packages (from requests->transformers->pytorch-widedeep) (2024.8.30)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\joann\\coding projects\\ipynb dump\\ipynb-dump\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.3.5->pytorch-widedeep) (1.16.0)\n",
      "Collecting pydantic-core==2.23.4\n",
      "  Using cached pydantic_core-2.23.4-cp39-none-win_amd64.whl (1.9 MB)\n",
      "Collecting annotated-types>=0.6.0\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\joann\\coding projects\\ipynb dump\\ipynb-dump\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy->pytorch-widedeep) (8.1.7)\n",
      "Collecting shellingham>=1.3.0\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\joann\\coding projects\\ipynb dump\\ipynb-dump\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy->pytorch-widedeep) (13.8.1)\n",
      "Collecting language-data>=1.2\n",
      "  Using cached language_data-1.2.0-py3-none-any.whl (5.4 MB)\n",
      "Collecting blis<0.8.0,>=0.7.8\n",
      "  Using cached blis-0.7.11-cp39-cp39-win_amd64.whl (6.6 MB)\n",
      "Collecting confection<1.0.0,>=0.0.1\n",
      "  Using cached confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0\n",
      "  Using cached cloudpathlib-0.19.0-py3-none-any.whl (49 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\joann\\coding projects\\ipynb dump\\ipynb-dump\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->pytorch-widedeep) (2.18.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\joann\\coding projects\\ipynb dump\\ipynb-dump\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->pytorch-widedeep) (3.0.0)\n",
      "Collecting marisa-trie>=0.7.7\n",
      "  Using cached marisa_trie-1.2.0-cp39-cp39-win_amd64.whl (152 kB)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\joann\\coding projects\\ipynb dump\\ipynb-dump\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->pytorch-widedeep) (0.1.2)\n",
      "Installing collected packages: cramjam, fastparquet, sentencepiece, sentence-transformers, opencv-contrib-python, pyarrow, spacy-legacy, pydantic-core, annotated-types, pydantic, cymem, shellingham, typer, catalogue, marisa-trie, language-data, langcodes, srsly, wasabi, blis, confection, murmurhash, preshed, thinc, spacy-loggers, smart-open, cloudpathlib, weasel, spacy, gensim, pytorch-widedeep\n",
      "Successfully installed annotated-types-0.7.0 blis-0.7.11 catalogue-2.0.10 cloudpathlib-0.19.0 confection-0.1.5 cramjam-2.8.4 cymem-2.0.8 fastparquet-2024.5.0 gensim-4.3.3 langcodes-3.4.1 language-data-1.2.0 marisa-trie-1.2.0 murmurhash-1.0.10 opencv-contrib-python-4.10.0.84 preshed-3.0.9 pyarrow-17.0.0 pydantic-2.9.2 pydantic-core-2.23.4 pytorch-widedeep-1.6.3 sentence-transformers-3.1.1 sentencepiece-0.2.0 shellingham-1.5.4 smart-open-7.0.4 spacy-3.7.6 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 thinc-8.2.5 typer-0.12.5 wasabi-1.1.3 weasel-0.4.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.3; however, version 24.2 is available.\n",
      "You should consider upgrading via the 'c:\\users\\joann\\coding projects\\ipynb dump\\ipynb-dump\\scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-widedeep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "lq0elU0J53Yo"
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "import os\n",
    "\n",
    "import random\n",
    "random.seed(SEED)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from pytorch_widedeep.preprocessing import TabPreprocessor\n",
    "from pytorch_widedeep.models import TabMlp, WideDeep\n",
    "from pytorch_widedeep import Trainer\n",
    "from pytorch_widedeep.metrics import R2Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aU3xdVpwzuLx"
   },
   "source": [
    "1.Divide the dataset (‘hdb_price_prediction.csv’) into train and test sets by using entries from the year 2020 and before as training data, and entries from 2021 and after as the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "_oYG6lNIh7Mp"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('hdb_price_prediction.csv')\n",
    "\n",
    "train_df = df[df['year'] <= 2020]  \n",
    "test_df = df[df['year'] >= 2021]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m_q9PoR50JAA"
   },
   "source": [
    "2.Refer to the documentation of Pytorch-WideDeep and perform the following tasks:\n",
    "https://pytorch-widedeep.readthedocs.io/en/latest/index.html\n",
    "* Use [**TabPreprocessor**](https://pytorch-widedeep.readthedocs.io/en/latest/examples/01_preprocessors_and_utils.html#2-tabpreprocessor) to create the deeptabular component using the continuous\n",
    "features and the categorical features. Use this component to transform the training dataset.\n",
    "* Create the [**TabMlp**](https://pytorch-widedeep.readthedocs.io/en/latest/pytorch-widedeep/model_components.html#pytorch_widedeep.models.tabular.mlp.tab_mlp.TabMlp) model with 2 linear layers in the MLP, with 200 and 100 neurons respectively.\n",
    "* Create a [**Trainer**](https://pytorch-widedeep.readthedocs.io/en/latest/pytorch-widedeep/trainer.html#pytorch_widedeep.training.Trainer) for the training of the created TabMlp model with the root mean squared error (RMSE) cost function. Train the model for 100 epochs using this trainer, keeping a batch size of 64. (Note: set the *num_workers* parameter to 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ZBY1iqUXtYWn"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joann\\Coding Projects\\ipynb dump\\ipynb-dump\\lib\\site-packages\\pytorch_widedeep\\preprocessing\\tab_preprocessor.py:360: UserWarning: Continuous columns will not be normalised\n",
      "  warnings.warn(\"Continuous columns will not be normalised\")\n",
      "epoch 1: 100%|██████████| 1366/1366 [00:10<00:00, 135.44it/s, loss=1.81e+5]\n",
      "epoch 2: 100%|██████████| 1366/1366 [00:22<00:00, 61.11it/s, loss=9.88e+4]\n",
      "epoch 3: 100%|██████████| 1366/1366 [00:11<00:00, 116.24it/s, loss=7.69e+4]\n",
      "epoch 4: 100%|██████████| 1366/1366 [00:15<00:00, 88.84it/s, loss=6.45e+4] \n",
      "epoch 5: 100%|██████████| 1366/1366 [00:09<00:00, 151.75it/s, loss=5.98e+4]\n",
      "epoch 6: 100%|██████████| 1366/1366 [00:08<00:00, 166.37it/s, loss=5.76e+4]\n",
      "epoch 7: 100%|██████████| 1366/1366 [00:07<00:00, 171.75it/s, loss=5.59e+4]\n",
      "epoch 8: 100%|██████████| 1366/1366 [00:08<00:00, 156.65it/s, loss=5.47e+4]\n",
      "epoch 9: 100%|██████████| 1366/1366 [00:08<00:00, 167.31it/s, loss=5.34e+4]\n",
      "epoch 10: 100%|██████████| 1366/1366 [00:08<00:00, 169.34it/s, loss=5.22e+4]\n",
      "epoch 11: 100%|██████████| 1366/1366 [00:08<00:00, 159.15it/s, loss=5.12e+4]\n",
      "epoch 12: 100%|██████████| 1366/1366 [00:08<00:00, 158.53it/s, loss=5.02e+4]\n",
      "epoch 13: 100%|██████████| 1366/1366 [00:09<00:00, 142.01it/s, loss=4.92e+4]\n",
      "epoch 14: 100%|██████████| 1366/1366 [00:10<00:00, 136.38it/s, loss=4.86e+4]\n",
      "epoch 15: 100%|██████████| 1366/1366 [00:10<00:00, 132.00it/s, loss=4.83e+4]\n",
      "epoch 16: 100%|██████████| 1366/1366 [00:10<00:00, 135.93it/s, loss=4.77e+4]\n",
      "epoch 17: 100%|██████████| 1366/1366 [00:10<00:00, 133.28it/s, loss=4.73e+4]\n",
      "epoch 18: 100%|██████████| 1366/1366 [00:10<00:00, 130.49it/s, loss=4.73e+4]\n",
      "epoch 19: 100%|██████████| 1366/1366 [00:10<00:00, 128.68it/s, loss=4.72e+4]\n",
      "epoch 20: 100%|██████████| 1366/1366 [00:10<00:00, 131.95it/s, loss=4.7e+4] \n",
      "epoch 21: 100%|██████████| 1366/1366 [00:10<00:00, 132.19it/s, loss=4.69e+4]\n",
      "epoch 22: 100%|██████████| 1366/1366 [00:10<00:00, 128.10it/s, loss=4.66e+4]\n",
      "epoch 23: 100%|██████████| 1366/1366 [00:10<00:00, 125.91it/s, loss=4.66e+4]\n",
      "epoch 24: 100%|██████████| 1366/1366 [00:10<00:00, 125.35it/s, loss=4.64e+4]\n",
      "epoch 25: 100%|██████████| 1366/1366 [00:11<00:00, 124.16it/s, loss=4.66e+4]\n",
      "epoch 26: 100%|██████████| 1366/1366 [00:10<00:00, 125.71it/s, loss=4.62e+4]\n",
      "epoch 27: 100%|██████████| 1366/1366 [00:10<00:00, 131.58it/s, loss=4.62e+4]\n",
      "epoch 28: 100%|██████████| 1366/1366 [00:10<00:00, 126.32it/s, loss=4.61e+4]\n",
      "epoch 29: 100%|██████████| 1366/1366 [00:11<00:00, 123.78it/s, loss=4.62e+4]\n",
      "epoch 30: 100%|██████████| 1366/1366 [00:11<00:00, 122.66it/s, loss=4.6e+4] \n",
      "epoch 31: 100%|██████████| 1366/1366 [00:11<00:00, 122.48it/s, loss=4.59e+4]\n",
      "epoch 32: 100%|██████████| 1366/1366 [00:10<00:00, 125.31it/s, loss=4.6e+4] \n",
      "epoch 33: 100%|██████████| 1366/1366 [00:11<00:00, 123.04it/s, loss=4.6e+4] \n",
      "epoch 34: 100%|██████████| 1366/1366 [00:10<00:00, 125.92it/s, loss=4.59e+4]\n",
      "epoch 35: 100%|██████████| 1366/1366 [00:11<00:00, 123.99it/s, loss=4.57e+4]\n",
      "epoch 36: 100%|██████████| 1366/1366 [00:10<00:00, 125.12it/s, loss=4.56e+4]\n",
      "epoch 37: 100%|██████████| 1366/1366 [00:11<00:00, 120.70it/s, loss=4.57e+4]\n",
      "epoch 38: 100%|██████████| 1366/1366 [00:10<00:00, 126.12it/s, loss=4.55e+4]\n",
      "epoch 39: 100%|██████████| 1366/1366 [00:10<00:00, 125.89it/s, loss=4.55e+4]\n",
      "epoch 40: 100%|██████████| 1366/1366 [00:11<00:00, 121.73it/s, loss=4.55e+4]\n",
      "epoch 41: 100%|██████████| 1366/1366 [00:11<00:00, 123.12it/s, loss=4.55e+4]\n",
      "epoch 42: 100%|██████████| 1366/1366 [00:11<00:00, 120.72it/s, loss=4.54e+4]\n",
      "epoch 43: 100%|██████████| 1366/1366 [00:10<00:00, 125.32it/s, loss=4.54e+4]\n",
      "epoch 44: 100%|██████████| 1366/1366 [00:10<00:00, 126.09it/s, loss=4.53e+4]\n",
      "epoch 45: 100%|██████████| 1366/1366 [00:11<00:00, 115.44it/s, loss=4.51e+4]\n",
      "epoch 46: 100%|██████████| 1366/1366 [00:11<00:00, 123.64it/s, loss=4.52e+4]\n",
      "epoch 47: 100%|██████████| 1366/1366 [00:11<00:00, 123.06it/s, loss=4.51e+4]\n",
      "epoch 48: 100%|██████████| 1366/1366 [00:11<00:00, 120.52it/s, loss=4.5e+4] \n",
      "epoch 49: 100%|██████████| 1366/1366 [00:11<00:00, 121.65it/s, loss=4.52e+4]\n",
      "epoch 50: 100%|██████████| 1366/1366 [00:11<00:00, 123.07it/s, loss=4.49e+4]\n",
      "epoch 51: 100%|██████████| 1366/1366 [00:11<00:00, 123.26it/s, loss=4.48e+4]\n",
      "epoch 52: 100%|██████████| 1366/1366 [00:11<00:00, 123.43it/s, loss=4.49e+4]\n",
      "epoch 53: 100%|██████████| 1366/1366 [00:11<00:00, 121.24it/s, loss=4.46e+4]\n",
      "epoch 54: 100%|██████████| 1366/1366 [00:11<00:00, 123.80it/s, loss=4.49e+4]\n",
      "epoch 55: 100%|██████████| 1366/1366 [00:10<00:00, 126.85it/s, loss=4.47e+4]\n",
      "epoch 56: 100%|██████████| 1366/1366 [00:10<00:00, 125.90it/s, loss=4.47e+4]\n",
      "epoch 57: 100%|██████████| 1366/1366 [00:11<00:00, 123.42it/s, loss=4.46e+4]\n",
      "epoch 58: 100%|██████████| 1366/1366 [00:10<00:00, 124.42it/s, loss=4.47e+4]\n",
      "epoch 59: 100%|██████████| 1366/1366 [00:10<00:00, 129.52it/s, loss=4.45e+4]\n",
      "epoch 60: 100%|██████████| 1366/1366 [00:10<00:00, 125.07it/s, loss=4.46e+4]\n",
      "epoch 61: 100%|██████████| 1366/1366 [00:11<00:00, 120.62it/s, loss=4.46e+4]\n",
      "epoch 62: 100%|██████████| 1366/1366 [00:11<00:00, 122.70it/s, loss=4.46e+4]\n",
      "epoch 63: 100%|██████████| 1366/1366 [00:11<00:00, 122.61it/s, loss=4.45e+4]\n",
      "epoch 64: 100%|██████████| 1366/1366 [00:11<00:00, 120.65it/s, loss=4.45e+4]\n",
      "epoch 65: 100%|██████████| 1366/1366 [00:10<00:00, 124.54it/s, loss=4.45e+4]\n",
      "epoch 66: 100%|██████████| 1366/1366 [00:11<00:00, 121.04it/s, loss=4.43e+4]\n",
      "epoch 67: 100%|██████████| 1366/1366 [00:11<00:00, 121.07it/s, loss=4.44e+4]\n",
      "epoch 68: 100%|██████████| 1366/1366 [00:11<00:00, 120.65it/s, loss=4.43e+4]\n",
      "epoch 69: 100%|██████████| 1366/1366 [00:11<00:00, 123.69it/s, loss=4.44e+4]\n",
      "epoch 70: 100%|██████████| 1366/1366 [00:11<00:00, 120.77it/s, loss=4.43e+4]\n",
      "epoch 71: 100%|██████████| 1366/1366 [00:11<00:00, 122.49it/s, loss=4.44e+4]\n",
      "epoch 72: 100%|██████████| 1366/1366 [00:11<00:00, 121.25it/s, loss=4.41e+4]\n",
      "epoch 73: 100%|██████████| 1366/1366 [00:11<00:00, 120.20it/s, loss=4.41e+4]\n",
      "epoch 74: 100%|██████████| 1366/1366 [00:11<00:00, 119.12it/s, loss=4.44e+4]\n",
      "epoch 75: 100%|██████████| 1366/1366 [00:11<00:00, 119.15it/s, loss=4.4e+4] \n",
      "epoch 76: 100%|██████████| 1366/1366 [00:11<00:00, 120.63it/s, loss=4.42e+4]\n",
      "epoch 77: 100%|██████████| 1366/1366 [00:11<00:00, 119.78it/s, loss=4.41e+4]\n",
      "epoch 78: 100%|██████████| 1366/1366 [00:10<00:00, 124.37it/s, loss=4.41e+4]\n",
      "epoch 79: 100%|██████████| 1366/1366 [00:11<00:00, 122.36it/s, loss=4.41e+4]\n",
      "epoch 80: 100%|██████████| 1366/1366 [00:10<00:00, 125.14it/s, loss=4.39e+4]\n",
      "epoch 81: 100%|██████████| 1366/1366 [00:11<00:00, 123.18it/s, loss=4.4e+4] \n",
      "epoch 82: 100%|██████████| 1366/1366 [00:11<00:00, 118.84it/s, loss=4.42e+4]\n",
      "epoch 83: 100%|██████████| 1366/1366 [00:11<00:00, 120.20it/s, loss=4.4e+4] \n",
      "epoch 84: 100%|██████████| 1366/1366 [00:11<00:00, 118.30it/s, loss=4.39e+4]\n",
      "epoch 85: 100%|██████████| 1366/1366 [00:11<00:00, 116.24it/s, loss=4.4e+4] \n",
      "epoch 86: 100%|██████████| 1366/1366 [00:11<00:00, 120.14it/s, loss=4.39e+4]\n",
      "epoch 87: 100%|██████████| 1366/1366 [00:11<00:00, 121.23it/s, loss=4.39e+4]\n",
      "epoch 88: 100%|██████████| 1366/1366 [00:11<00:00, 121.68it/s, loss=4.38e+4]\n",
      "epoch 89: 100%|██████████| 1366/1366 [00:11<00:00, 119.48it/s, loss=4.39e+4]\n",
      "epoch 90: 100%|██████████| 1366/1366 [00:11<00:00, 117.90it/s, loss=4.38e+4]\n",
      "epoch 91: 100%|██████████| 1366/1366 [00:12<00:00, 113.21it/s, loss=4.39e+4]\n",
      "epoch 92: 100%|██████████| 1366/1366 [00:11<00:00, 120.68it/s, loss=4.4e+4] \n",
      "epoch 93: 100%|██████████| 1366/1366 [00:11<00:00, 115.19it/s, loss=4.4e+4] \n",
      "epoch 94: 100%|██████████| 1366/1366 [00:11<00:00, 122.63it/s, loss=4.38e+4]\n",
      "epoch 95: 100%|██████████| 1366/1366 [00:11<00:00, 120.91it/s, loss=4.38e+4]\n",
      "epoch 96: 100%|██████████| 1366/1366 [00:11<00:00, 118.30it/s, loss=4.37e+4]\n",
      "epoch 97: 100%|██████████| 1366/1366 [00:11<00:00, 121.09it/s, loss=4.38e+4]\n",
      "epoch 98: 100%|██████████| 1366/1366 [00:11<00:00, 121.75it/s, loss=4.36e+4]\n",
      "epoch 99: 100%|██████████| 1366/1366 [00:11<00:00, 122.51it/s, loss=4.36e+4]\n",
      "epoch 100: 100%|██████████| 1366/1366 [00:11<00:00, 118.60it/s, loss=4.35e+4]\n"
     ]
    }
   ],
   "source": [
    "categorical_cols = ['month', 'town', 'flat_model_type', 'storey_range']  \n",
    "continuous_cols = ['dist_to_nearest_stn', 'dist_to_dhoby', 'degree_centrality', 'eigenvector_centrality', 'remaining_lease_years', 'floor_area_sqm'] \n",
    "\n",
    "\n",
    "# Create the TabPreprocessor\n",
    "tab_preprocessor = TabPreprocessor(\n",
    "    cat_embed_cols=categorical_cols,\n",
    "    continuous_cols= continuous_cols\n",
    ")\n",
    "X_tab = tab_preprocessor.fit_transform(train_df)\n",
    "\n",
    "# Build the TabMlp model\n",
    "tabmlp = TabMlp(\n",
    "    mlp_hidden_dims=[200, 100],  \n",
    "    column_idx=tab_preprocessor.column_idx, \n",
    "    cat_embed_input=tab_preprocessor.cat_embed_input,  \n",
    "    continuous_cols=continuous_cols \n",
    ")\n",
    "\n",
    "# Create the WideDeep model\n",
    "model = WideDeep(deeptabular=tabmlp)\n",
    "\n",
    "# Create the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,  \n",
    "    cost_function=\"rmse\",  \n",
    "    num_workers=0  \n",
    ")\n",
    "\n",
    "no_epochs = 100\n",
    "batch_size = 64\n",
    "\n",
    "trainer.fit(\n",
    "    X_tab=X_tab,  \n",
    "    target=train_df['resale_price'].values,  \n",
    "    n_epochs=no_epochs,  \n",
    "    batch_size=batch_size  \n",
    ")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V46s-MdM0y5c"
   },
   "source": [
    "3.Report the test RMSE and the test R2 value that you obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "KAhAgvMC07g6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "predict: 100%|██████████| 1128/1128 [00:02<00:00, 396.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE & R2\n",
      "Test RMSE: 100470.89891734322\n",
      "Test R2: 0.6473214017550503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "c:\\Users\\joann\\Coding Projects\\ipynb dump\\ipynb-dump\\lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#TODO: Check res orng\" gmna\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "X_tab_test = tab_preprocessor.transform(test_df)\n",
    "\n",
    "y_pred = trainer.predict(X_tab=X_tab_test)\n",
    "y_true = test_df['resale_price'] \n",
    "\n",
    "print('RMSE & R2')\n",
    "\n",
    "rmse = mean_squared_error(y_true, y_pred, squared=False)  # Set squared=False to get the RMSE\n",
    "print(f'Test RMSE: {rmse}')\n",
    "\n",
    "r2 = r2_score(y_true, y_pred) \n",
    "print(f'Test R2: {r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
